{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Tuple, Dict, Any, Set, Callable, Hashable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\n",
    "  # 'rec_id',\n",
    "  'given_name',\n",
    "  'surname',\n",
    "  'street_number',\n",
    "  'address_1',\n",
    "  'address_2',\n",
    "  'suburb',\n",
    "  # 'postcode',\n",
    "  'state',\n",
    "  'date_of_birth',\n",
    "  # 'age',\n",
    "  'phone_number',\n",
    "  # 'soc_sec_id',\n",
    "  # 'blocking_number'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_address(address: str) -> str:\n",
    "    # Список слов и сокращений для удаления\n",
    "    words_to_remove = [\n",
    "        \"улица\", \"ул\\\\. ?\", \"проспект\", \"пр-т\\\\. ?\", \"пр\\\\. ?\", \"бульвар\", \"б-р\\\\. ?\", \"переулок\", \"пер\\\\. ?\", \"набережная\", \"наб\\\\. ?\",\n",
    "        \"шоссе\", \"площадь\", \"пл\\\\. ?\", \"дом\", \"д\\\\. ?\", \"квартира\", \"кв\\\\. ?\", \"корпус\", \"корп\\\\. ?\", \"строение\", \"стр\\\\. ?\", \"область\",\n",
    "        \"обл\\\\. ?\", \"город\", \"г\\\\. ?\", \"поселок\", \"пос\\\\. ?\", \"деревня\", \"дер\\\\. ?\",\n",
    "        \"street\", \"st\\\\. ?\", \"avenue\", \"ave\\\\. ?\", \"boulevard\", \"blvd\\\\. ?\", \"alley\", \"al\\\\. ?\", \"drive\", \"dr\\\\. ?\",\n",
    "        \"square\", \"sq\\\\. ?\", \"house\", \"h\\\\. ?\", \"apartment\", \"apt\\\\. ?\", \"building\", \"bldg\\\\. ?\", \"county\", \"co\\\\. ?\",\n",
    "        \"city\", \"ct\\\\. ?\", \"village\", \"vil\\\\. ?\", \"township\", \"twp\\\\. ?\", \"road\", \"rd\\\\. ?\"\n",
    "    ]\n",
    "\n",
    "    # Создаем регулярное выражение из списка слов и сокращений\n",
    "    pattern = r'\\b(?:{})\\b'.format('|'.join(words_to_remove))\n",
    "\n",
    "    # Заменяем найденные слова и сокращения на пустую строку\n",
    "    cleaned_address = re.sub(pattern, '', address, flags=re.IGNORECASE)\n",
    "\n",
    "    # Удаляем лишние пробелы и возвращаем очищенную строку\n",
    "    return re.sub(r'\\s+', ' ', cleaned_address).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(x: str) -> str:\n",
    "    # Формируем ключ из значений полей\n",
    "    return  '-'.join([x.split('-')[0], x.split('-')[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('test.csv', dtype=str)\n",
    "data = data.applymap(lambda x:  x.strip() if isinstance(x, str) else x )\n",
    "# data['address_1'] = data['address_1'].apply(lambda x:  clean_address(x) if isinstance(x, str) else x )\n",
    "# data['address_2'] = data['address_2'].apply(lambda x:  clean_address(x) if isinstance(x, str) else x )\n",
    "# data = data.applymap(lambda x:  clean_address(x) if isinstance(x, str) else x )\n",
    "data = data.replace({np.nan:None})\n",
    "data['rec_common_id'] = data.apply(lambda x: get_key(x['rec_id']), axis=1)\n",
    "data_list = data.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_ids = {}\n",
    "expected_res = {}\n",
    "for record in data_list:\n",
    "    if record['rec_common_id'] not in data_by_ids:\n",
    "        data_by_ids[record['rec_common_id']] = []\n",
    "        expected_res[record['rec_common_id']] = set()\n",
    "    data_by_ids[record['rec_common_id']].append(record)\n",
    "    expected_res[record['rec_common_id']].add(record['rec_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distances.levenshtein import levenstein_similarity\n",
    "from distances.jaro import jaro_winkler_similarity\n",
    "from distances.damerau_levenstein import damerau_levenshtein_similarity\n",
    "from distances.jaccard import jaccard_similarity_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_mean(a: Dict[Hashable, Any], b: Dict[Hashable, Any], sim: Callable[[str, str], float]) -> float:\n",
    "  sm = 0\n",
    "  cnt = 0\n",
    "  for k in column_names:\n",
    "    if a[k] is not None and b[k] is not None and isinstance(a[k], str) and a[k] != '' and isinstance(b[k], str) and b[k] != '':\n",
    "      sm += sim(a[k], b[k])\n",
    "      cnt += 1\n",
    "  if cnt == 0:\n",
    "    return 0\n",
    "  return sm / cnt\n",
    "def get_lev_sim(a: Dict[Hashable, Any], b: Dict[Hashable, Any]) -> float:\n",
    "  return get_sim_mean(a, b, levenstein_similarity)\n",
    "def get_dam_lev_sim(a: Dict[Hashable, Any], b: Dict[Hashable, Any]) -> float:\n",
    "  return get_sim_mean(a, b, damerau_levenshtein_similarity)\n",
    "def get_jaro_sim(a: Dict[Hashable, Any], b: Dict[Hashable, Any]) -> float:\n",
    "  return get_sim_mean(a, b, jaro_winkler_similarity)\n",
    "def get_jaccard_sim(a: Dict[Hashable, Any], b: Dict[Hashable, Any]) -> float:\n",
    "  return get_sim_mean(a, b, jaccard_similarity_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_rec_id: rec-148-dup-2, td_rec_id: rec-148-dup-0, similarity: 0.6753246753246753\n",
      "main_rec_id: rec-148-dup-2, td_rec_id: rec-148-org, similarity: 0.6854256854256854\n",
      "main_rec_id: rec-148-dup-2, td_rec_id: rec-148-dup-1, similarity: 0.613997113997114\n",
      "main_rec_id: rec-148-dup-2, td_rec_id: rec-148-dup-3, similarity: 0.608044733044733\n",
      "main_rec_id: rec-148-dup-2, td_rec_id: rec-100-dup-3, similarity: 0.188973063973064\n"
     ]
    }
   ],
   "source": [
    "test_data = data_by_ids['rec-148']\n",
    "main_rec = test_data[-1]\n",
    "test_data = test_data[:-1]\n",
    "test_data.append(data_by_ids['rec-100'][0])\n",
    "for td in test_data:\n",
    "  sim = get_lev_sim(main_rec, td)\n",
    "  print(f'main_rec_id: {main_rec[\"rec_id\"]}, td_rec_id: {td[\"rec_id\"]}, similarity: {sim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in column_names:\n",
    "#   if main_rec[k] is not None and isinstance(main_rec[k], str):\n",
    "#     print(f'{k}: {main_rec[k]}')\n",
    "# for k in column_names:\n",
    "#   if test_data[2][k] is not None and isinstance(test_data[2][k], str):\n",
    "#     print(f'{k}: {test_data[2][k]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_distances(data: List[Dict[Hashable, Any]], expected_res: Dict[str, Set[str]], similarity: Callable[[Dict[Hashable, Any], Dict[Hashable, Any]], float],threshold: float = 0.85) -> Tuple[int, int, int]:\n",
    "  tp = 0\n",
    "  fp = 0\n",
    "  fn = 0\n",
    "  res = []\n",
    "  for i in range(len(data)):\n",
    "    for j in range(i + 1, len(data)):\n",
    "      sim = similarity(data[i], data[j])\n",
    "      if sim > threshold:\n",
    "        res.append((data[i]['rec_id'], data[j]['rec_id'], sim))\n",
    "  res_by_ids = {}\n",
    "  for (a_id, b_id, sim) in res:\n",
    "    a_key = get_key(a_id)\n",
    "    b_key = get_key(b_id)\n",
    "    if a_key != b_key:\n",
    "      print(f'ERROR: {a_id}, {b_id}, {sim}')\n",
    "      fp += 1\n",
    "      continue\n",
    "    if a_key not in res_by_ids:\n",
    "      res_by_ids[a_key] = set()\n",
    "    res_by_ids[a_key].add(a_id)\n",
    "    res_by_ids[a_key].add(b_id)\n",
    "  for key in expected_res:\n",
    "    if key not in res_by_ids:\n",
    "      if len(expected_res[key]) > 1:\n",
    "        # print(f'ERROR: not found {key}')\n",
    "        fn += len(expected_res[key])\n",
    "      continue\n",
    "    # if len(expected_res[key]) != len(res_by_ids[key]):\n",
    "    #   print(f'ERROR: not full {key}')\n",
    "    fn += len(expected_res[key]) - len(res_by_ids[key])\n",
    "    tp += len(res_by_ids[key])\n",
    "  return tp, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEVENSTEIN\n",
      "tp: 429, fp: 0, fn: 122\n",
      "precision: 1.0\n",
      "recall: 0.778584392014519\n",
      "f1: 0.8755102040816326\n"
     ]
    }
   ],
   "source": [
    "(tp, fp, fn) = test_distances(data_list, expected_res, get_lev_sim)\n",
    "print('LEVENSTEIN')\n",
    "print(f'tp: {tp}, fp: {fp}, fn: {fn}')\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "print(f'precision: {precision}')\n",
    "print(f'recall: {recall}')\n",
    "print(f'f1: {2 * precision * recall / (precision + recall)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JARO\n",
      "tp: 469, fp: 0, fn: 82\n",
      "precision: 1.0\n",
      "recall: 0.8511796733212341\n",
      "f1: 0.919607843137255\n"
     ]
    }
   ],
   "source": [
    "(tp, fp, fn) = test_distances(data_list, expected_res, get_jaro_sim)\n",
    "print('JARO')\n",
    "print(f'tp: {tp}, fp: {fp}, fn: {fn}')\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "print(f'precision: {precision}')\n",
    "print(f'recall: {recall}')\n",
    "print(f'f1: {2 * precision * recall / (precision + recall)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAMERAU LEVENSTEIN\n",
      "tp: 433, fp: 0, fn: 118\n",
      "precision: 1.0\n",
      "recall: 0.7858439201451906\n",
      "f1: 0.8800813008130082\n"
     ]
    }
   ],
   "source": [
    "(tp, fp, fn) = test_distances(data_list, expected_res, get_dam_lev_sim)\n",
    "print('DAMERAU LEVENSTEIN')\n",
    "print(f'tp: {tp}, fp: {fp}, fn: {fn}')\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "print(f'precision: {precision}')\n",
    "print(f'recall: {recall}')\n",
    "print(f'f1: {2 * precision * recall / (precision + recall)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JACCARD\n",
      "tp: 435, fp: 0, fn: 116\n",
      "precision: 1.0\n",
      "recall: 0.7894736842105263\n",
      "f1: 0.8823529411764706\n"
     ]
    }
   ],
   "source": [
    "(tp, fp, fn) = test_distances(data_list, expected_res, get_jaccard_sim)\n",
    "print('JACCARD')\n",
    "print(f'tp: {tp}, fp: {fp}, fn: {fn}')\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "print(f'precision: {precision}')\n",
    "print(f'recall: {recall}')\n",
    "print(f'f1: {2 * precision * recall / (precision + recall)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kroiakaeschpovunmolak -> 5915477668\n",
      "kroiagaeschbofumnalak -> 5915477668\n",
      "kroiakaeschpovunmolak -> 5915477668\n",
      "kroiagaeschbofumnalak -> 5915477668\n"
     ]
    }
   ],
   "source": [
    "from dcs import dcs_plus_plus\n",
    "from dm_soundex import encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcs_key(r: Dict[Hashable, Any]) -> str:\n",
    "  name = r['given_name']\n",
    "  sur = r['surname']\n",
    "  if name is None:\n",
    "    name = ''\n",
    "  # elif name != '':\n",
    "  #   name = encode(name, max_length=10, zero_pad=True)\n",
    "  if sur is None:\n",
    "    sur = ''\n",
    "  # elif sur != '':\n",
    "  #   sur = encode(sur, max_length=10, zero_pad=True)\n",
    "  s = f'{name}{sur}'\n",
    "  if s == '':\n",
    "    return '0'*10\n",
    "  # print(encode(r['surname'], max_length=10, zero_pad=True))\n",
    "  return s\n",
    "def is_duplicate(r1: Dict[Hashable, Any], r2: Dict[Hashable, Any]) -> bool:\n",
    "  return get_jaro_sim(r1, r2) > 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c5e99a60-c3a8-48e0-a14b-94992d2aab14', '2c5e98be-5122-4e58-9e46-86c48ef302b3', 'b2c91609-4fa8-458e-9cb3-944063fa2e50', 'a2d32b94-24cb-4e20-8717-a6a7a62dcd9c', '6f0ddfec-ac6f-4ae0-9c94-cbd08ed0ae53', '72aae635-6f85-4220-8ad9-6a527f6ddec5', '412605f3-680a-41a3-8234-8d4cba0747e8', '738fb943-7f2b-4b64-809d-85a6f42a4b14', '8a35c9eb-f29d-4443-97c6-b6016cf91eb2', '39409a63-e99e-49bf-9b86-43c002c30557']\n"
     ]
    }
   ],
   "source": [
    "res = dcs_plus_plus(data_list, dcs_key, is_duplicate=is_duplicate, w=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 440, fp: 0, fn: 111\n",
      "precision: 1.0\n",
      "recall: 0.7985480943738656\n",
      "f1: 0.887991927346115\n"
     ]
    }
   ],
   "source": [
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "res_by_ids = {}\n",
    "for (a, b) in res:\n",
    "  a_id = a['rec_id']\n",
    "  b_id = b['rec_id']\n",
    "  if a_id == b_id:\n",
    "    print(f'ERROR: {a_id}, {b_id}')\n",
    "    continue\n",
    "  a_key = get_key(a_id)\n",
    "  b_key = get_key(b_id)\n",
    "  if a_key != b_key:\n",
    "    # print(f'ERROR: {a_id}, {b_id}, {sim}')\n",
    "    fp += 1\n",
    "    continue\n",
    "  if a_key not in res_by_ids:\n",
    "    res_by_ids[a_key] = set()\n",
    "  res_by_ids[a_key].add(a_id)\n",
    "  res_by_ids[a_key].add(b_id)\n",
    "for key in expected_res:\n",
    "  if key not in res_by_ids:\n",
    "    if len(expected_res[key]) > 1:\n",
    "      # print(f'ERROR: not found {key}')\n",
    "      fn += len(expected_res[key])\n",
    "    continue\n",
    "  # if len(expected_res[key]) != len(res_by_ids[key]):\n",
    "  #   print(f'ERROR: not full {key}')\n",
    "  fn += len(expected_res[key]) - len(res_by_ids[key])\n",
    "  tp += len(res_by_ids[key])\n",
    "print(f'tp: {tp}, fp: {fp}, fn: {fn}')\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "print(f'precision: {precision}')\n",
    "print(f'recall: {recall}')\n",
    "print(f'f1: {2 * precision * recall / (precision + recall)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
